{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity\n",
    "\n",
    "https://medium.com/@Intellica.AI/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (2020.4.4)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (4.45.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all NLTK files\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample non ascii cenia remove stopwords punctuations'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "def pre_process(corpus):\n",
    "    # convert input corpus to lower case.\n",
    "    corpus = corpus.lower()\n",
    "    # collecting a list of stop words from nltk and punctuation form\n",
    "    # string class and create single array.\n",
    "    stopset = stopwords.words('english') + list(string.punctuation)\n",
    "    # remove stop words and punctuations from string.\n",
    "    # word_tokenize is used to tokenize the input corpus in word tokens.\n",
    "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
    "    # remove non-ascii characters\n",
    "    corpus = unidecode(corpus)\n",
    "    \n",
    "\n",
    "    return corpus\n",
    "pre_process(\"Sample of non ASCII: Ceñía. How to remove stopwords and punctuations?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The striped bats are hanging on their feet for best',\n",
       " 'The striped bat are hanging on their foot for best')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "words = word_tokenize(sentence)\n",
    "# for w in words:\n",
    "#     print(w, \" : \", lemmatizer.lemmatize(w))\n",
    "\n",
    "    \n",
    "# word_tokenize is used to tokenize the input corpus in word tokens.\n",
    "new_sentence = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(sentence)])\n",
    "\n",
    "\n",
    "sentence, new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# sentence pair\n",
    "corpus = [\"A girl is styling her hair.\", \"A girl is brushing her hair.\"]\n",
    "for c in range(len(corpus)):\n",
    "    corpus[c] = pre_process(corpus[c])\n",
    "# creating vocabulary using uni-gram and bi-gram\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "feature_vectors = tfidf_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim tutorial\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "# Download trained word2vec model from Google here\n",
    "# https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# give a path of model to load function\n",
    "word_emb_model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "word_emb_model = gensim.models.KeyedVectors.load_word2vec_format('./browser-qa-app/browser-qa-api/model/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIF\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def map_word_frequency(document):\n",
    "    return Counter(itertools.chain(*document))\n",
    "    \n",
    "def get_sif_feature_vectors(sentence1, sentence2, word_emb_model=word_emb_model):\n",
    "    sentence1 = [token for token in sentence1.split() if token in word_emb_model.vocab]\n",
    "    sentence2 = [token for token in sentence2.split() if token in word_emb_model.vocab]\n",
    "    \n",
    "    word_counts = map_word_frequency((sentence1 + sentence2))\n",
    "    embedding_size = 300 # size of vectore in word embeddings\n",
    "    a = 0.001\n",
    "    sentence_set=[]\n",
    "    for sentence in [sentence1, sentence2]:\n",
    "        vs = np.zeros(embedding_size)\n",
    "        sentence_length = len(sentence)\n",
    "        for word in sentence:\n",
    "            a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n",
    "        \n",
    "        \n",
    "        # TODO: Fix me\n",
    "#         print(vs)\n",
    "#         print(sentence_length)\n",
    "        vs = np.divide(vs, sentence_length) # weighted average\n",
    "        sentence_set.append(vs)\n",
    "    return sentence_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"A girl is styling her hair.\"\n",
    "sentence2 = \"A girl is brushing her hair.\"\n",
    "sentence3 = \"A boy is playing football\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('girl styling hair', 'girl brushing hair', 'boy playing football')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentence1 = pre_process(sentence1)\n",
    "processed_sentence2 = pre_process(sentence2)\n",
    "processed_sentence3 = pre_process(sentence3)\n",
    "\n",
    "processed_sentence1, processed_sentence2, processed_sentence3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-d2024c27208c>:21: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n"
     ]
    }
   ],
   "source": [
    "sentence_set1 = get_sif_feature_vectors(processed_sentence1, processed_sentence2)\n",
    "sentence_set2 = get_sif_feature_vectors(processed_sentence1, processed_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7876443533464236"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cosine_similarity(sentence_set1[0], sentence_set1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33389045197895445"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cosine_similarity(sentence_set2[0], sentence_set2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
